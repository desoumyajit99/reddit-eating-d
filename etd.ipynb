{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9aa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "import csv\n",
    "\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "from prawcore.exceptions import NotFound, Forbidden, ServerError, ResponseException, RequestException \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9470e1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_only: True\n"
     ]
    }
   ],
   "source": [
    "# Enter your Reddit API credentials here\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"YOUR_CLIENT_ID\",\n",
    "    client_secret=\"YOUR_CLIENT_SECRET\",\n",
    "    user_agent=\"script:ed-scraper:0.1 (by u/YOUR_USERNAME)\",\n",
    ")\n",
    "\n",
    "# Check if the Reddit instance is read-only\n",
    "print(\"read_only:\", reddit.read_only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e48f481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert a timestamp to ISO format\n",
    "def iso(ts):\n",
    "    return datetime.fromtimestamp(ts, tz=timezone.utc).isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29725d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to estimate upvotes and downvotes from score and upvote ratio\n",
    "def approx_votes(score, ratio):\n",
    "    \"\"\"\n",
    "    Estimate ups and downs from score and upvote_ratio.\n",
    "    WARNING: This is approximate; Reddit fuzzes/withholds data.\n",
    "    Returns (ups, downs) or (None, None) if not computable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if ratio is None or ratio == 0.5:\n",
    "            return None, None\n",
    "        n = score / (2 * ratio - 1)\n",
    "        ups = int(round(ratio * n))\n",
    "        downs = int(round((1 - ratio) * n))\n",
    "        if ups < 0 or downs < 0:\n",
    "            return None, None\n",
    "        return ups, downs\n",
    "    except ZeroDivisionError:\n",
    "        return None, None\n",
    "    except Exception:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c19c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely convert author to string, handling None\n",
    "def safe_author(a):\n",
    "    return str(a) if a is not None else \"[deleted]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c6daa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch a submission with its comments, expanding as needed\n",
    "def fetch_submission_with_comments(subm, expand_more=8, max_comments=300):\n",
    "    \"\"\"\n",
    "    expand_more: how many MoreComments nodes to expand (None=all, can be huge)\n",
    "    max_comments: cap the total number of flattened comments we store\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"id\": subm.id,\n",
    "        \"title\": subm.title,\n",
    "        \"is_self\": subm.is_self,\n",
    "        \"selftext\": subm.selftext if subm.is_self else \"\",\n",
    "        \"url\": subm.url,\n",
    "        \"permalink\": f\"https://www.reddit.com{subm.permalink}\",\n",
    "        \"subreddit\": str(subm.subreddit.display_name),\n",
    "        \"author\": safe_author(subm.author),\n",
    "        \"created_utc\": subm.created_utc,\n",
    "        \"created_iso\": iso(subm.created_utc),\n",
    "        \"num_comments\": subm.num_comments,\n",
    "        \"score\": subm.score,\n",
    "        \"upvote_ratio\": getattr(subm, \"upvote_ratio\", None),\n",
    "        \"approx_upvotes\": None,\n",
    "        \"approx_downvotes\": None,\n",
    "        \"comments\": []\n",
    "    }\n",
    "\n",
    "    # Estimate upvotes and downvotes\n",
    "    ups, downs = approx_votes(subm.score, getattr(subm, \"upvote_ratio\", None))\n",
    "    data[\"approx_upvotes\"] = ups\n",
    "    data[\"approx_downvotes\"] = downs\n",
    "\n",
    "    # Expand some comment trees \n",
    "    try:\n",
    "        subm.comments.replace_more(limit=expand_more)\n",
    "        flat = subm.comments.list()\n",
    "    except Exception as e:\n",
    "        # If expansion fails, just use whatever we have\n",
    "        flat = []\n",
    "        print(f\"  ! Could not expand comments for {subm.id}: {e}\")\n",
    "\n",
    "    # Cap total number of comments to store\n",
    "    if max_comments is not None:\n",
    "        flat = flat[:max_comments]\n",
    "\n",
    "    for c in flat:\n",
    "        if isinstance(c, MoreComments):\n",
    "            continue\n",
    "        try:\n",
    "            data[\"comments\"].append({\n",
    "                \"id\": c.id,\n",
    "                \"author\": safe_author(c.author),\n",
    "                \"body\": c.body,\n",
    "                \"created_utc\": c.created_utc,\n",
    "                \"created_iso\": iso(c.created_utc),\n",
    "                \"score\": c.score,\n",
    "                \"parent_id\": c.parent_id,\n",
    "                \"link_id\": c.link_id\n",
    "            })\n",
    "        except Exception as e:\n",
    "            # Some comments may fail to serialize (e.g., deleted/removed)\n",
    "            print(f\"    ! Skipping a comment in {subm.id}: {e}\")\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3730fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect authors from a subreddit \n",
    "seed_subreddit = \"EDanonymemes\"  # Change this to your target subreddit\n",
    "authors = set()\n",
    "for submission in reddit.subreddit(seed_subreddit).new(limit=40):\n",
    "    a = safe_author(submission.author)\n",
    "    if a not in (\"[deleted]\", \"None\", \"AutoModerator\"):\n",
    "        authors.add(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55cd6cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 37 authors from r/EDanonymemes\n"
     ]
    }
   ],
   "source": [
    "# Sort authors to have a consistent order\n",
    "authors = sorted(authors)\n",
    "print(f\"Collected {len(authors)} authors from r/{seed_subreddit}\")\n",
    "\n",
    "with open(\"authors.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(authors, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee0539c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Map authors to subreddits (as you had), and collect post/comment data \n",
    "user_subreddits = {}\n",
    "user_posts = {}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ab43c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/37] Fetching for u/-mosaicaxolotl- ...\n",
      "\n",
      "[2/37] Fetching for u/Ashamed_Ad8162 ...\n",
      "\n",
      "[3/37] Fetching for u/BloomingInTheVoid ...\n",
      "\n",
      "[4/37] Fetching for u/Correct_Fig_6198 ...\n",
      "\n",
      "[5/37] Fetching for u/Entire_Weather3209 ...\n",
      "\n",
      "[6/37] Fetching for u/GuilefulEyes ...\n",
      "\n",
      "[7/37] Fetching for u/Impurest_Vessel ...\n",
      "\n",
      "[8/37] Fetching for u/KlausMikaelsonsWife ...\n",
      "\n",
      "[9/37] Fetching for u/LaaaaMaaaa ...\n",
      "\n",
      "[10/37] Fetching for u/New-Desk1419 ...\n",
      "\n",
      "[11/37] Fetching for u/No_Astronomer656 ...\n",
      "\n",
      "[12/37] Fetching for u/Quirky-Reception7087 ...\n",
      "\n",
      "[13/37] Fetching for u/Snap-Crackle-Plop- ...\n",
      "\n",
      "[14/37] Fetching for u/Squidd_Vicious ...\n",
      "\n",
      "[15/37] Fetching for u/WoebegoneWoodlouse ...\n",
      "\n",
      "[16/37] Fetching for u/_AroAce_in_space_ ...\n",
      "\n",
      "[17/37] Fetching for u/acoolrock ...\n",
      "\n",
      "[18/37] Fetching for u/alexisseffy ...\n",
      "\n",
      "[19/37] Fetching for u/aqua4cry ...\n",
      "\n",
      "[20/37] Fetching for u/birb-jesus ...\n",
      "\n",
      "[21/37] Fetching for u/cupidhurts ...\n",
      "\n",
      "[22/37] Fetching for u/funkydyke ...\n",
      "\n",
      "[23/37] Fetching for u/garje ...\n",
      "\n",
      "[24/37] Fetching for u/hunktycrunkty ...\n",
      "\n",
      "[25/37] Fetching for u/icanttkaeitanymore ...\n",
      "\n",
      "[26/37] Fetching for u/lackingneitherhat ...\n",
      "\n",
      "[27/37] Fetching for u/loverboi3 ...\n",
      "\n",
      "[28/37] Fetching for u/mmmheadphones ...\n",
      "\n",
      "[29/37] Fetching for u/pessimistic_witch ...\n",
      "\n",
      "[30/37] Fetching for u/questbarsryummy ...\n",
      "\n",
      "[31/37] Fetching for u/rabidloving ...\n",
      "\n",
      "[32/37] Fetching for u/stevenator3162 ...\n",
      "\n",
      "[33/37] Fetching for u/untakenusername42658 ...\n",
      "\n",
      "[34/37] Fetching for u/uselespieceofshi02 ...\n",
      "\n",
      "[35/37] Fetching for u/vi-lavender ...\n",
      "\n",
      "[36/37] Fetching for u/wompwomp_e ...\n",
      "\n",
      "[37/37] Fetching for u/xoxoAnniMuxoxo ...\n"
     ]
    }
   ],
   "source": [
    "# Adjust the server load for ethical crawling\n",
    "MAX_SUBMISSIONS_PER_USER = 50\n",
    "EXPAND_MORE_COMMENTS = 8     # None = all (can explode), 0 = only already-loaded\n",
    "MAX_COMMENTS_PER_POST = 300  # cap\n",
    "PAUSE_BETWEEN_USERS = 1.0    # seconds, to be nice to the API\n",
    "\n",
    "for i, username in enumerate(authors, 1):\n",
    "    print(f\"\\n[{i}/{len(authors)}] Fetching for u/{username} ...\")\n",
    "    sub_list = []\n",
    "    posts_list = []\n",
    "    try:\n",
    "        redditor = reddit.redditor(username)\n",
    "        # Iterate through this user's recent submissions\n",
    "        for subm in redditor.submissions.new(limit=MAX_SUBMISSIONS_PER_USER):\n",
    "            sname = str(subm.subreddit.display_name)\n",
    "            if sname not in sub_list:\n",
    "                sub_list.append(sname)\n",
    "\n",
    "            # Build rich post + comments record\n",
    "            try:\n",
    "                post_data = fetch_submission_with_comments(\n",
    "                    subm,\n",
    "                    expand_more=EXPAND_MORE_COMMENTS,\n",
    "                    max_comments=MAX_COMMENTS_PER_POST\n",
    "                )\n",
    "                posts_list.append(post_data)\n",
    "            except (Forbidden, NotFound) as e:\n",
    "                print(f\"  ! Skipping a post (private/forbidden/not found): {e}\")\n",
    "            except (ServerError, ResponseException, RequestException) as e:\n",
    "                print(f\"  ! Transient error on a post: {e}\")\n",
    "                time.sleep(2)\n",
    "\n",
    "        user_subreddits[username] = sub_list\n",
    "        user_posts[username] = {\"submissions\": posts_list}\n",
    "\n",
    "    except (Forbidden, NotFound) as e:\n",
    "        print(f\"Skipping u/{username}: {e}\")\n",
    "    except (ServerError, ResponseException, RequestException) as e:\n",
    "        print(f\"Transient error for u/{username}: {e}\")\n",
    "        time.sleep(2)\n",
    "    finally:\n",
    "        time.sleep(PAUSE_BETWEEN_USERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cce4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results in JSON files\n",
    "with open(\"user_subreddits.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_subreddits, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"user_posts_with_comments.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(user_posts, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\n",
    "    f\"\\nSaved subreddit mapping for {len(user_subreddits)} users \"\n",
    "    f\"and rich post+comment data to user_posts_with_comments.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f0780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exports written:\n",
      "  - crawl_combined.csv\n",
      "  - posts_only.csv\n",
      "  - comments_only.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results in CSV files\n",
    "# CSV export (combined + separate) \n",
    "COMBINED_CSV = \"crawl_combined.csv\"\n",
    "POSTS_CSV = \"posts_only.csv\"\n",
    "COMMENTS_CSV = \"comments_only.csv\"\n",
    "\n",
    "# Predefine consistent column orders\n",
    "post_cols = [\n",
    "    \"seed_user\",              # the user whose profile we crawled\n",
    "    \"post_id\",\n",
    "    \"post_title\",\n",
    "    \"post_is_self\",\n",
    "    \"post_selftext\",\n",
    "    \"post_url\",\n",
    "    \"post_permalink\",\n",
    "    \"post_subreddit\",\n",
    "    \"post_author\",\n",
    "    \"post_created_utc\",\n",
    "    \"post_created_iso\",\n",
    "    \"post_score\",\n",
    "    \"post_upvote_ratio\",\n",
    "    \"post_approx_upvotes\",\n",
    "    \"post_approx_downvotes\",\n",
    "    \"post_num_comments\",\n",
    "]\n",
    "comment_cols = [\n",
    "    \"comment_id\",\n",
    "    \"comment_author\",\n",
    "    \"comment_body\",\n",
    "    \"comment_created_utc\",\n",
    "    \"comment_created_iso\",\n",
    "    \"comment_score\",\n",
    "    \"comment_parent_id\",\n",
    "    \"comment_link_id\",\n",
    "]\n",
    "combined_cols = [\"record_type\"] + post_cols + comment_cols  # record_type = 'post' or 'comment'\n",
    "\n",
    "combined_rows = []\n",
    "post_rows = []\n",
    "comment_rows = []\n",
    "\n",
    "for seed_user, pdata in user_posts.items():\n",
    "    for post in pdata.get(\"submissions\", []):\n",
    "        # ---- Post row\n",
    "        p_row = {\n",
    "            \"seed_user\": seed_user,\n",
    "            \"post_id\": post.get(\"id\"),\n",
    "            \"post_title\": post.get(\"title\"),\n",
    "            \"post_is_self\": post.get(\"is_self\"),\n",
    "            \"post_selftext\": post.get(\"selftext\"),\n",
    "            \"post_url\": post.get(\"url\"),\n",
    "            \"post_permalink\": post.get(\"permalink\"),\n",
    "            \"post_subreddit\": post.get(\"subreddit\"),\n",
    "            \"post_author\": post.get(\"author\"),\n",
    "            \"post_created_utc\": post.get(\"created_utc\"),\n",
    "            \"post_created_iso\": post.get(\"created_iso\"),\n",
    "            \"post_score\": post.get(\"score\"),\n",
    "            \"post_upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "            \"post_approx_upvotes\": post.get(\"approx_upvotes\"),\n",
    "            \"post_approx_downvotes\": post.get(\"approx_downvotes\"),\n",
    "            \"post_num_comments\": post.get(\"num_comments\"),\n",
    "        }\n",
    "        post_rows.append(p_row)\n",
    "\n",
    "        # Add to combined with empty comment fields\n",
    "        combined_rows.append(\n",
    "            {\"record_type\": \"post\", **p_row,\n",
    "             **{k: \"\" for k in comment_cols}}\n",
    "        )\n",
    "\n",
    "        # ---- Comment rows for this post\n",
    "        for c in post.get(\"comments\", []):\n",
    "            c_row = {\n",
    "                # comment fields\n",
    "                \"comment_id\": c.get(\"id\"),\n",
    "                \"comment_author\": c.get(\"author\"),\n",
    "                \"comment_body\": c.get(\"body\"),\n",
    "                \"comment_created_utc\": c.get(\"created_utc\"),\n",
    "                \"comment_created_iso\": c.get(\"created_iso\"),\n",
    "                \"comment_score\": c.get(\"score\"),\n",
    "                \"comment_parent_id\": c.get(\"parent_id\"),\n",
    "                \"comment_link_id\": c.get(\"link_id\"),\n",
    "                # keep post context alongside each comment (useful in 1-file analysis)\n",
    "                \"seed_user\": seed_user,\n",
    "                \"post_id\": post.get(\"id\"),\n",
    "                \"post_title\": post.get(\"title\"),\n",
    "                \"post_is_self\": post.get(\"is_self\"),\n",
    "                \"post_selftext\": post.get(\"selftext\"),\n",
    "                \"post_url\": post.get(\"url\"),\n",
    "                \"post_permalink\": post.get(\"permalink\"),\n",
    "                \"post_subreddit\": post.get(\"subreddit\"),\n",
    "                \"post_author\": post.get(\"author\"),\n",
    "                \"post_created_utc\": post.get(\"created_utc\"),\n",
    "                \"post_created_iso\": post.get(\"created_iso\"),\n",
    "                \"post_score\": post.get(\"score\"),\n",
    "                \"post_upvote_ratio\": post.get(\"upvote_ratio\"),\n",
    "                \"post_approx_upvotes\": post.get(\"approx_upvotes\"),\n",
    "                \"post_approx_downvotes\": post.get(\"approx_downvotes\"),\n",
    "                \"post_num_comments\": post.get(\"num_comments\"),\n",
    "            }\n",
    "            # For the separate comments-only file, keep only comment columns\n",
    "            comment_rows.append({k: c_row[k] for k in comment_cols})\n",
    "\n",
    "            # For the combined file, align to combined_cols\n",
    "            combined_rows.append(\n",
    "                {\n",
    "                    \"record_type\": \"comment\",\n",
    "                    **{k: c_row.get(k) for k in post_cols},\n",
    "                    **{k: c_row.get(k) for k in comment_cols},\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Write CSVs\n",
    "def write_csv(path, fieldnames, rows):\n",
    "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction=\"ignore\")\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "# posts_only.csv\n",
    "write_csv(POSTS_CSV, post_cols, post_rows)\n",
    "\n",
    "# comments_only.csv\n",
    "write_csv(COMMENTS_CSV, comment_cols, comment_rows)\n",
    "\n",
    "# crawl_combined.csv (posts + comments in one long table)\n",
    "write_csv(COMBINED_CSV, combined_cols, combined_rows)\n",
    "\n",
    "print(f\"CSV exports written:\\n  - {COMBINED_CSV}\\n  - {POSTS_CSV}\\n  - {COMMENTS_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eatingd-BBfbkbyC-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
